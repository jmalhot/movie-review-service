{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2aab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning Sentiment Analysis Model for Movie Reviews\n",
    "\n",
    "This script demonstrates how to fine-tune a DistilBERT model for sentiment analysis\n",
    "specifically on movie reviews using the IMDB dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c953597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff45e7c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091c6a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load IMDB dataset and prepare it for training.\"\"\"\n",
    "    logging.info(\"Loading IMDB dataset...\")\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    \n",
    "    print(f\"Dataset format: {dataset}\")\n",
    "    print(f\"Training samples: {len(dataset['train'])}\")\n",
    "    print(f\"Testing samples: {len(dataset['test'])}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bfbc2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \"\"\"Initialize the model and tokenizer.\"\"\"\n",
    "    model_checkpoint = \"distilbert-base-uncased\"\n",
    "    logging.info(f\"Loading model and tokenizer from {model_checkpoint}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=2  # Binary classification (positive/negative)\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a58f66",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, tokenizer):\n",
    "    \"\"\"Preprocess and tokenize the dataset.\"\"\"\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    logging.info(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bee44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Calculate accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels,\n",
    "        preds,\n",
    "        average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3bff4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_training(model, tokenizer, tokenized_dataset):\n",
    "    \"\"\"Set up training arguments and initialize trainer.\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f9415",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(trainer):\n",
    "    \"\"\"Train the model and evaluate performance.\"\"\"\n",
    "    logging.info(\"Starting model training...\")\n",
    "    train_results = trainer.train()\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(\"Training metrics:\")\n",
    "    print(train_results.metrics)\n",
    "    \n",
    "    logging.info(\"Evaluating model performance...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return train_results, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05b601",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, output_dir=\"../models/fine_tuned_sentiment\"):\n",
    "    \"\"\"Save the fine-tuned model and tokenizer.\"\"\"\n",
    "    logging.info(f\"Saving model to {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a092d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer):\n",
    "    \"\"\"Test the model with sample reviews.\"\"\"\n",
    "    def predict_sentiment(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0][prediction].item()\n",
    "        \n",
    "        return {\n",
    "            \"sentiment\": \"POSITIVE\" if prediction == 1 else \"NEGATIVE\",\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "    \n",
    "    sample_reviews = [\n",
    "        \"This movie was absolutely fantastic! The acting was superb and the story was engaging throughout.\",\n",
    "        \"I was really disappointed with this film. The plot was confusing and the pacing was too slow.\",\n",
    "        \"An average movie with some good moments but nothing spectacular. The acting was decent.\",\n",
    "        \"The special effects were amazing, but the story lacked depth and character development.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing model with sample reviews:\\n\")\n",
    "    for review in sample_reviews:\n",
    "        result = predict_sentiment(review)\n",
    "        print(f\"Review: {review}\")\n",
    "        print(f\"Sentiment: {result['sentiment']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d92b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load and prepare dataset\n",
    "    dataset = load_and_prepare_data()\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model, tokenizer = initialize_model()\n",
    "    \n",
    "    # Preprocess data\n",
    "    tokenized_dataset = preprocess_data(dataset, tokenizer)\n",
    "    \n",
    "    # Setup training\n",
    "    trainer = setup_training(model, tokenizer, tokenized_dataset)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    train_results, eval_results = train_model(trainer)\n",
    "    \n",
    "    # Save model\n",
    "    save_model(model, tokenizer)\n",
    "    \n",
    "    # Test model\n",
    "    test_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
